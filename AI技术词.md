# AI技术词

## AI
旨在让计算机系统去模拟人类的智能，从而解决问题和完成任务

### 机器学习(ML, Machine Learning)是AI的子集
不需要做显式编程，让计算机通过算法，自行学习和改进，去识别模式、做出预测和决策。
比如：喂给花的图片去识别玫瑰和向日葵

#### 监督学习(UL, Unsupervised Learning)是ML的子集
机器学习会接受有标签的训练数据，标签就是期望的输出值，即包括有输入的特征，也包含期望的输出值，算法的目标是学习输入和输出之间的映射关系
经典的监督学习任务包括分类和回归。分类即把数据划分为不同的类别（猫狗分类），回归即对数值进行预测（预测房价）。

#### 无监督学习(SL, Supervised Learning)是ML的子集
无监督学习中的数据没有标签，需要自主发现数据里的模式或规律。
经典的无监督学习包括聚类。聚类即把数据进行分组，根据特征把相似内容进行组织。

#### 强化学习(RL, Reinforcement Learning)是ML的子集
强化学习是让模型在环境里采取行动，获得结果反馈，再从反馈里学习，从而在给定情况下采取最佳行动，来最大化行动或最小化损失。

#### 深度学习(DL, Deep Learning)是ML的子集，和UL,SL,RL的交集

##### 生成式AI(Generative AI)是DL的子集
##### 大语言模型(LLM, Large Language Model)是DL的子集和生成式AI有交集
不是所有的AIGC即生成式AI都是LLM，所有的LLM也并不一定都是生成式AI，比如谷歌的BERT模型。BERT在理解上下文的能力较强，因此被谷歌用在搜索上来提高搜索排名和信息摘录的准确性，也被用于情感分析、文本分类等任务，但同时不擅长文本生成。

### Transformer大模型
编码器用来理解和表示输入序列，解码器用来生成输出序列。
Token
Attention，自注意力机制

仅编码器/自编码模型 encoder-only/autoencoding model
例子：BERT，掩码语言建模、情感分析等
仅解码器/自回归模型 decoder-only/autoregressive model
例子：ChatGPT，文本生成
编码器-解码器/序列到序列模型 encoder-decoder/sequence-to-sequence model
例子：T5，BART， 翻译、总结


### ChatGPT如何炼成
#### 1.无监督与训练
通过大量的文本进行无监督学习预训练，得到一个能进行文本生成的基座模型
#### 2.监督微调
通过一些人类撰写的高质量对话数据，对基座模型进行一个监督微调，得到一个微调后的模型，此模型具备更好的对话能力
#### 3.训练奖励模型+强化学习训练
通过问问题和对个对应回答的数据，让人类标注员对回答进行质量排序，然后基于这些数据训练出一个能回答进行评分预测的奖励模型；
让第二步得到的模型对问题生成回答，用奖励模型给回答进行评分，利用评分反馈进行强化学习训练。

